---
title: "bootstrapping"
output: github_document
    
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(rvest)
library(lme4)
library(modelr)
library(mgcv)
library(broom.mixed)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## 1. Load packages and set seed

```{r}
library(tidyverse)
library(p8105.datasets)

set.seed(1)
```

What this does:
- `tidyverse` → data cleaning + plotting
- `p8105.datasets` → class datasets
- `set.seed(1)` → makes random results identical each time

# What is bootstrapping?

In the real world, you only get one sample. Bootstrapping lets you resample your sample, with replacement, many times.
Each resample = “fake dataset” that mimics taking a new sample from the population.
You fit your model to each resample.
The variability across all bootstrap samples = the uncertainty of your estimates.

## 2. Simulated data (constant vs. non-constant variance)

```{r}
n_samp = 250
```

## create data with constant variance
```{r}
sim_df_const = 
  tibble(
    x = rnorm(n_samp, 1, 1),
    error = rnorm(n_samp, 0, 1),
    y = 2 + 3 * x + error
  )
```

## create data with non-constant variance
```{r}
sim_df_nonconst = sim_df_const |> 
  mutate(
    error = error * .75 * x,
    y = 2 + 3 * x + error
)
```

Meaning:
- Both datasets follow the same regression line
- But the noise changes in the second dataset depending on x
- (Non-constant variance = heteroscedasticity)

## Plot the two datasets
```{r}
sim_df = 
  bind_rows(const = sim_df_const, nonconst = sim_df_nonconst, .id = "data_source") 

sim_df |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm") +
  facet_grid(~data_source)
```

Shows:
- left panel = constant variance
- right panel = non-constant variance

## 3. Fit ordinary linear regression to both datasets
```{r}
lm(y ~ x, data = sim_df_const) |> broom::tidy()
lm(y ~ x, data = sim_df_nonconst) |> broom::tidy()
```

Observation:
Even though variances are different, standard errors look similar, not good.
We need a better way -> bootstrap.

## BOOTSTRAPPING
## 4. Function to draw ONE bootstrap sample
```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}
```

Meaning:
- Takes a dataset
- Randomly samples rows with replacement
- Sample size stays the same

## Check one bootstrap sample
```{r}
boot_sample(sim_df_nonconst) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm")
```

Shows:
- similar shape
- some repeated points
- some missing points

## 5. Create 1,000 bootstrap samples
```{r}
boot_straps = 
  tibble(strap_number = 1:1000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = sim_df_nonconst))
  )
```

What this does:
- Makes a tibble of 1000 rows
- Each row = a bootstrap sample stored in a list column

## Inspect first 3 samples
```{r}
boot_straps |> 
  slice(1:3) |> 
  mutate(strap_sample = map(strap_sample, \(s) arrange(s, x))) |> 
  pull(strap_sample)
```

shows:
- repeated rows
- missing rows
- different structure each time

## Plot first 3 bootstrap samples with their regression lines

```{r}
boot_straps |> 
  slice(1:3) |> 
  unnest(strap_sample) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_point(alpha = .5) +
  stat_smooth(method = "lm", se = FALSE) +
  facet_grid(~strap_number)
```

Shows how bootstrap samples differ.

## Analyze ALL bootstrap samples
## 6. Fit regression to every bootstrap sample (+ tidy)
```{r}
bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(y ~ x, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-strap_sample, -models) |> 
  unnest(results)
```

This:
- fits 1000 regressions
- extracts coefficients
- binds them into one big dataset

## Compute bootstrap standard errors
```{r}
bootstrap_results |> 
  group_by(term) |> 
  summarize(boot_se = sd(estimate))
```
Bootstrap SEs reflect the unusual variance pattern.

## Bootstrap 95% confidence intervals
```{r}
bootstrap_results |> 
  group_by(term) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975))
```

This uses the empirical bootstrap distribution.

## Visualize regression lines from bootstrap samples
```{r}
boot_straps |> 
  unnest(strap_sample) |> 
  ggplot(aes(x = x, y = y)) + 
  geom_line(aes(group = strap_number), stat = "smooth", method = "lm", se = FALSE, alpha = .1, color = "blue") +
  geom_point(data = sim_df_nonconst, alpha = .5)
```

Interpretation:
- lines bunch together around x = 0 → smaller variance
- lines spread at extremes → larger variance
This matches the data generation.

## Faster bootstrapping using `modelr::bootstrap()`
## 7. Automatic bootstrapping

```{r}
boot_straps = 
  sim_df_nonconst |> 
  modelr::bootstrap(n = 1000)
```

This creates resample objects (saves memory).

## view one bootstrap sample
```{r}
boot_straps |> pull(strap) |> nth(1) |> as_tibble()
```

## full bootstrap pipeline (automatic version)
```{r}
sim_df_nonconst |> 
  modelr::bootstrap(n = 1000) |> 
  mutate(
    models = map(strap, \(df) lm(y ~ x, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-strap, -models) |> 
  unnest(results) |> 
  group_by(term) |> 
  summarize(boot_se = sd(estimate))
```

Same result as before, but cleaner.

## Do the same for constant-variance dataset
```{r}
sim_df_const |> 
  modelr::bootstrap(n = 1000) |> 
  mutate(
    models = map(strap, \(df) lm(y ~ x, data = df)),
    results = map(models, broom::tidy)) |> 
  select(-strap, -models) |> 
  unnest(results) |> 
  group_by(term) |> 
  summarize(boot_se = sd(estimate))
```

## AIRBNB Example
## 8. Load and clean data
```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb |> 
  mutate(stars = review_scores_location / 2) |> 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) |> 
  filter(borough != "Staten Island") |> 
  drop_na(price, stars) |> 
  select(price, stars, borough, neighborhood, room_type)
```

## Quick plot
```{r}
nyc_airbnb |> 
  ggplot(aes(x = stars, y = price, color = room_type)) + 
  geom_point() 
```

Outliers in price are clearly visible.

## Bootstrap regression coefficients for Manhattan listings
```{r}
nyc_airbnb |> 
  filter(borough == "Manhattan") |> 
  modelr::bootstrap(n = 1000) |> 
  mutate(
    models = map(strap, \(df) lm(price ~ stars + room_type, data = df)),
    results = map(models, broom::tidy)) |> 
  select(results) |> 
  unnest(results) |> 
  filter(term == "stars") |> 
  ggplot(aes(x = estimate)) + geom_density()
```

Meaning:
- we look at the distribution of the star-rating coefficient
- the distribution has heavy tails
- caused by high-price outliers appearing in some bootstrap samples






































































